{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re \n",
    "import pandas as pd\n",
    "import string\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.corpus import stopwords  # Remove useless words\n",
    "from nltk.stem.lancaster import LancasterStemmer  # Convert words to base form; aggressive\n",
    "\n",
    "# Import packages that help us to create document-term matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Stopwords: words which are of no use for our project\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords are a list of 'useless' words\n",
    "set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load The data and display the head, i.e the first 5 rows.\n",
    "data = pd.read_csv('train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info() #to check if there are any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check percentage of comments that are toxic compared to normal comments\n",
    "data.toxic.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new subset of the data by only taking the 2nd column onwards (comments and categories)\n",
    "data_count=data.iloc[:,2:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a chart with the following size\n",
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "# Plot a bar chart using the index (category values) and the count of each category. alpha = 0.8 to make the bars more translucent\n",
    "ax = sns.barplot(data_count.index, data_count.values, alpha=0.8)\n",
    "\n",
    "plt.title(\"No. of comments per class\")\n",
    "plt.ylabel('No. of Occurrences', fontsize=12)\n",
    "plt.xlabel('Type ', fontsize=12)\n",
    "\n",
    "#adding the text labels for each bar\n",
    "rects = ax.patches\n",
    "labels = data_count.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = len(data)\n",
    "print(num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar graph \n",
    "sum_tox = data['toxic'].sum() / num_rows * 100\n",
    "sum_sev = data['severe_toxic'].sum() / num_rows * 100\n",
    "sum_obs = data['obscene'].sum() / num_rows * 100\n",
    "sum_thr = data['threat'].sum() / num_rows * 100\n",
    "sum_ins = data['insult'].sum() / num_rows * 100\n",
    "sum_ide = data['identity_hate'].sum() / num_rows * 100\n",
    "\n",
    "# Initiate a list of 6 values that represent the 6 x-axis values for the categories\n",
    "ind = np.arange(6)\n",
    "\n",
    "# Let the ind variable be the x-axis, whereas the % of toxicity for each category be the y-axis.\n",
    "# Sequence of % have been sorted manually. This method cannot be done if there are large numbers of categories.\n",
    "ax = plt.barh(ind, [sum_tox, sum_obs, sum_ins, sum_sev, sum_ide, sum_thr])\n",
    "plt.xlabel('Percentage (%)', size=20)\n",
    "plt.xticks(np.arange(0, 30, 5), size=20)\n",
    "plt.title('% of comments in various categories', size=22)\n",
    "plt.yticks(ind, ('Toxic', 'Obscene', 'Insult', 'Severe Toxic', 'Identity Hate', 'Threat', ), size=15)\n",
    "\n",
    "# Invert the graph so that it is in descending order.\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing steps - remove numbers, capital letters, punctuation, '\\n'\n",
    "import re\n",
    "import string\n",
    "\n",
    "# remove all numbers with letters attached to them\n",
    "alphanumeric = lambda x: re.sub('\\w*\\d\\w*', ' ', x)\n",
    "\n",
    "# '[%s]' % re.escape(string.punctuation),' ' - replace punctuation with white space\n",
    "# .lower() - convert all strings to lowercase \n",
    "punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())\n",
    "\n",
    "# Remove all '\\n' in the string and replace it with a space\n",
    "remove_n = lambda x: re.sub(\"\\n\", \" \", x)\n",
    "\n",
    "# Remove all non-ascii characters \n",
    "remove_non_ascii = lambda x: re.sub(r'[^\\x00-\\x7f]',r' ', x)\n",
    "\n",
    "# Apply all the lambda functions wrote previously through .map on the comments column\n",
    "data['comment_text'] = data['comment_text'].map(alphanumeric).map(punc_lower).map(remove_n).map(remove_non_ascii)\n",
    "\n",
    "data['comment_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tox = data.loc[:,['id','comment_text','toxic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tox['comment_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tox['comment_text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sev = data.loc[:,['id','comment_text','severe_toxic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_obs = data.loc[:,['id','comment_text','obscene']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_thr = data.loc[:,['id','comment_text','threat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ins = data.loc[:,['id','comment_text','insult']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ide = data.loc[:,['id','comment_text','identity_hate']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating WordCloud\n",
    "### Useful to show the words which occur most frequently for each category. Warning: Profanity ahead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud(df, label):\n",
    "    \n",
    "    # Print only rows where the toxic category label value is 1 (ie. the comment is toxic)\n",
    "    subset=df[df[label]==1]\n",
    "    text=subset.comment_text.values\n",
    "    wc= WordCloud(background_color=\"black\",max_words=4000)\n",
    "\n",
    "    wc.generate(\" \".join(text))\n",
    "\n",
    "    plt.figure(figsize=(20,20))\n",
    "    plt.subplot(221)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Words frequented in {}\".format(label), fontsize=20)\n",
    "    plt.imshow(wc.recolor(colormap= 'gist_earth' , random_state=244), alpha=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud(data_ide,'identity_hate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud(data_ins, 'insult')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Earlier, we saw that comments that are toxic (and other forms of toxicity) make up less than 10% of the comments in the data. This leads to the issue of class imbalance.\n",
    "\n",
    "#### We can deal with class imbalance by taking a subset of the data where the proportion of the toxic comments are at least 20% (ideally 50%) in relation to non-toxic comments.\n",
    "\n",
    "#### For a start, we can take 5000 rows of comments that are toxic and concatenate them row-wise with those that are not toxic so that we have a balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tox.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tox_1 = data_tox[data_tox['toxic'] == 1].iloc[0:5000,:]\n",
    "data_tox_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tox_0 = data_tox[data_tox['toxic'] == 0].iloc[0:5000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tox_done = pd.concat([data_tox_1, data_tox_0], axis=0)\n",
    "data_tox_done.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not all categories have 5000 rows. So we should count them first and make them balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sev[data_sev['severe_toxic'] == 1].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sev_1 = data_sev[data_sev['severe_toxic'] == 1].iloc[0:1595,:]\n",
    "data_sev_0 = data_sev[data_sev['severe_toxic'] == 0].iloc[0:1595,:]\n",
    "data_sev_done = pd.concat([data_sev_1, data_sev_0], axis=0)\n",
    "data_sev_done.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool. We only had 1,595 comments that are severely toxic. We combine it together with another 1,595 comments that are not toxic to form a new dataset that is balanced. We repeat this for all other categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_obs[data_obs['obscene'] == 1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_obs_1 = data_obs[data_obs['obscene'] == 1].iloc[0:5000,:]\n",
    "data_obs_0 = data_obs[data_obs['obscene'] == 0].iloc[0:5000,:]\n",
    "data_obs_done = pd.concat([data_obs_1, data_obs_0], axis=0)\n",
    "data_obs_done.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_thr[data_thr['threat'] == 1].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of threat comments of 478 is too miniscule when addded with another 478 clean comments for a proper analysis. We decided that the clean comments will comprise 80% at the most of the dataset, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_thr_1 = data_thr[data_thr['threat'] == 1].iloc[0:478,:]\n",
    "\n",
    "# We include 1912 comments that have no threat so that the data with threat (478) will represent 20% of the dataset.\n",
    "data_thr_0 = data_thr[data_thr['threat'] == 0].iloc[0:1912,:]  \n",
    "data_thr_done = pd.concat([data_thr_1, data_thr_0], axis=0)\n",
    "data_thr_done.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ins[data_ins['insult'] == 1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ins_1 = data_ins[data_ins['insult'] == 1].iloc[0:5000,:]\n",
    "data_ins_0 = data_ins[data_ins['insult'] == 0].iloc[0:5000,:]\n",
    "data_ins_done = pd.concat([data_ins_1, data_ins_0], axis=0)\n",
    "data_ins_done.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ide[data_ide['identity_hate'] == 1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ide_1 = data_ide[data_ide['identity_hate'] == 1].iloc[0:1405,:] # 20%\n",
    "data_ide_0 = data_ide[data_ide['identity_hate'] == 0].iloc[0:5620,:] # 80%\n",
    "data_ide_done = pd.concat([data_ide_1, data_ide_0], axis=0)\n",
    "data_ide_done.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder: Number of comments that fall into the following categories:\n",
    "\n",
    "- Toxic (14,000+) \n",
    "- Severe Toxic (1595)\n",
    "- Obscene (8449)\n",
    "- Threat (478)\n",
    "- Insult (7877)\n",
    "- Identity Hate (1405)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### df_****_done refers to the dataframes of each class that has been balanced (at least 20/80 proportion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                      | DF name       | No. of pts (1) | No. of pts (0) | Total data pts |\n",
    "|----------------------|---------------|----------------|----------------|----------------|\n",
    "| Toxic                | data_tox_done | 5000           | 5000           | 10000          |\n",
    "| Severe Toxic         | data_sev_done | 1595           | 1595           | 3190           |\n",
    "| Obscene (8449)       | data_obs_done | 5000           | 5000           | 10000          |\n",
    "| Threat (478)         | data_thr_done | 478            | 1912           | 2390           |\n",
    "| Insult (7877)        | data_ins_done | 5000           | 5000           | 10000          |\n",
    "| Identity Hate (1405) | data_ide_done | 1405           | 5620           | 7025           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages for pre-processing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Import tools to split data and evaluate model performance\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, precision_recall_curve, fbeta_score, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# Import ML algos\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create simple function that takes in a dataset and allows user to choose dataset, toxicity label, vectorizer and number of ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_done: data_tox_done, data_sev_done, ...\n",
    "label: toxic, severe_toxic, ...\n",
    "vectorizer values: CountVectorizer, TfidfVectorizer\n",
    "gram_range values: (1,1) for unigram, (2,2) for bigram\n",
    "'''\n",
    "def cv_tf_train_test(df_done,label,vectorizer,ngram):\n",
    "\n",
    "    ''' Train/Test split'''\n",
    "    # Split the data into X and y data sets\n",
    "    X = df_done.comment_text\n",
    "    y = df_done[label]\n",
    "\n",
    "    # Split our data into training and test data \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    ''' Count Vectorizer/TF-IDF '''\n",
    "\n",
    "    # Create a Vectorizer object and remove stopwords from the table\n",
    "    cv1 = vectorizer(ngram_range=(ngram), stop_words='english')\n",
    "    \n",
    "    X_train_cv1 = cv1.fit_transform(X_train) # Learn the vocabulary dictionary and return term-document matrix\n",
    "    X_test_cv1  = cv1.transform(X_test)      # Learn a vocabulary dictionary of all tokens in the raw documents.\n",
    "    \n",
    "    # Output a Dataframe of the CountVectorizer with unique words as the labels\n",
    "    # test = pd.DataFrame(X_train_cv1.toarray(), columns=cv1.get_feature_names())\n",
    "        \n",
    "    ''' Initialize all model objects and fit the models on the training data '''\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_train_cv1, y_train)\n",
    "    print('lr done')\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(X_train_cv1, y_train)\n",
    "    print('knn done')\n",
    "\n",
    "    bnb = BernoulliNB()\n",
    "    bnb.fit(X_train_cv1, y_train)\n",
    "    print('bnb done')\n",
    "    \n",
    "    mnb = MultinomialNB()\n",
    "    mnb.fit(X_train_cv1, y_train)\n",
    "    print('mnb done')\n",
    "    \n",
    "    svm_model = LinearSVC()\n",
    "    svm_model.fit(X_train_cv1, y_train)\n",
    "\n",
    "    randomforest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    randomforest.fit(X_train_cv1, y_train)\n",
    "    print('rdf done')\n",
    "    \n",
    "    # Create a list of F1 score of all models, F1 score of F measure is (2*(precision*recall)/(precision+recall))\n",
    "    f1_score_data = {'F1 Score':[f1_score(lr.predict(X_test_cv1), y_test), f1_score(knn.predict(X_test_cv1), y_test), \n",
    "                                f1_score(bnb.predict(X_test_cv1), y_test), f1_score(mnb.predict(X_test_cv1), y_test),\n",
    "                                f1_score(svm_model.predict(X_test_cv1), y_test), f1_score(randomforest.predict(X_test_cv1), y_test)]} \n",
    "                          \n",
    "    # Create DataFrame with the model names as column labels\n",
    "    df_f1 = pd.DataFrame(f1_score_data, index=['Log Regression','KNN', 'BernoulliNB', 'MultinomialNB', 'SVM', 'Random Forest'])  \n",
    "\n",
    "    return df_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create a TF-IDF vectorizer object for each category and calculate the F1 scores across all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def cv_tf_train_test(df_done,label,vectorizer,ngram)\n",
    "vectorizer values: CountVectorizer, TfidfVectorizer\n",
    "ngram_range values: (1,1) for unigram, (2,2) for bigram\n",
    "'''\n",
    "\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "df_tox_cv = cv_tf_train_test(data_tox_done, 'toxic', TfidfVectorizer, (1,1))\n",
    "df_tox_cv.rename(columns={'F1 Score': 'F1 Score(toxic)'}, inplace=True)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "total = 'Time taken: {} seconds'.format(t1-t0)\n",
    "print(total)\n",
    "\n",
    "df_tox_cv\n",
    "\n",
    "# Various permutations of the dataset, category, vectorizer and n-gram\n",
    "\n",
    "# cv_tf_train_test(data_tox_done, 'toxic', CountVectorizer, (1,1))\n",
    "# cv_tf_train_test(data_sev_done, 'severe_toxic', CountVectorizer, (1,1))\n",
    "# cv_tf_train_test(data_obs_done, 'obscene', CountVectorizer, (1,1))\n",
    "# cv_tf_train_test(data_thr_done, 'threat', CountVectorizer, (1,1))\n",
    "# cv_tf_train_test(data_ins_done, 'insult', CountVectorizer, (1,1))\n",
    "# cv_tf_train_test(data_ide_done, 'identity_hate', CountVectorizer, (1,1))\n",
    "\n",
    "# cv_tf_train_test(data_tox_done, 'toxic', TfidfVectorizer, (1,1))\n",
    "# cv_tf_train_test(data_sev_done, 'severe_toxic', TfidfVectorizer, (1,1))\n",
    "# cv_tf_train_test(data_obs_done, 'obscene', TfidfVectorizer, (1,1))\n",
    "# cv_tf_train_test(data_thr_done, 'threat', TfidfVectorizer, (1,1))\n",
    "# cv_tf_train_test(data_ins_done, 'insult', TfidfVectorizer, (1,1))\n",
    "# cv_tf_train_test(data_ide_done, 'identity_hate', TfidfVectorizer, (1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t0 = time.time()\n",
    "\n",
    "df_sev_cv = cv_tf_train_test(data_sev_done, 'severe_toxic', TfidfVectorizer, (1,1))\n",
    "df_sev_cv.rename(columns={'F1 Score': 'F1 Score(severe_toxic)'}, inplace=True)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "total = 'Time taken: {} seconds'.format(t1-t0)\n",
    "print(total)\n",
    "df_sev_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t0 = time.time()\n",
    "\n",
    "df_obs_cv = cv_tf_train_test(data_obs_done, 'obscene', TfidfVectorizer, (1,1))\n",
    "df_obs_cv.rename(columns={'F1 Score': 'F1 Score(obscene)'}, inplace=True)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "total = 'Time taken: {} seconds'.format(t1-t0)\n",
    "print(total)\n",
    "\n",
    "df_obs_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t0 = time.time()\n",
    "\n",
    "df_thr_cv = cv_tf_train_test(data_thr_done, 'threat', TfidfVectorizer, (1,1))\n",
    "df_thr_cv.rename(columns={'F1 Score': 'F1 Score(threat)'}, inplace=True)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "total = 'Time taken: {} seconds'.format(t1-t0)\n",
    "print(total)\n",
    "\n",
    "df_thr_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t0 = time.time()\n",
    "\n",
    "df_ins_cv = cv_tf_train_test(data_ins_done, 'insult', TfidfVectorizer, (1,1))\n",
    "df_ins_cv.rename(columns={'F1 Score': 'F1 Score(insult)'}, inplace=True)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "total = 'Time taken: {} seconds'.format(t1-t0)\n",
    "print(total)\n",
    "\n",
    "df_ins_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t0 = time.time()\n",
    "\n",
    "df_ide_cv = cv_tf_train_test(data_ide_done, 'identity_hate', TfidfVectorizer, (1,1))\n",
    "df_ide_cv.rename(columns={'F1 Score': 'F1 Score(identity_hate)'}, inplace=True)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "total = 'Time taken: {} seconds'.format(t1-t0)\n",
    "print(total)\n",
    "\n",
    "df_ide_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's combine the dataframes into a master dataframe to compare F1 scores across all categories.\n",
    "f1_all = pd.concat([df_tox_cv, df_sev_cv, df_obs_cv, df_ins_cv, df_thr_cv, df_ide_cv], axis=1)\n",
    "f1_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transpose the combined F1 dataframe to make it suitable for presentation on a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_all_trp = f1_all.transpose()\n",
    "f1_all_trp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=f1_all_trp, size=[10,10], markers=True)\n",
    "plt.xticks(rotation='90', fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend(loc='best')\n",
    "plt.title('F1 Score of ML models (TF-IDF)', fontsize=20)\n",
    "\n",
    "# Repeat this for CountVectorizer as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearSVC and Random Forest models perform best (purple and brown lines seem to be the highest)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test if our code actually works. Probability of the comment falling in various categories should be output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tox_done.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_tox_done.comment_text\n",
    "y = data_tox_done['toxic']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initiate a Tfidf vectorizer\n",
    "tfv = TfidfVectorizer(ngram_range=(1,1), stop_words='english')\n",
    "\n",
    "X_train_fit = tfv.fit_transform(X_train)  # Convert the X data into a document term matrix dataframe\n",
    "X_test_fit = tfv.transform(X_test)  # Converts the X_test comments into Vectorized format\n",
    "\n",
    "randomforest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train our SVM model with the X training data converted into Count Vectorized format with the Y training data\n",
    "randomforest.fit(X_train_fit, y_train)\n",
    "randomforest.predict(X_test_fit)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Prediction\n",
    "comment1 = ['You piece of shit']\n",
    "comment2 = ['What is up garden apple doing']\n",
    "\n",
    "comment1_vect = tfv.transform(comment1)\n",
    "randomforest.predict_proba(comment1_vect)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment2_vect = tfv.transform(comment2)\n",
    "randomforest.predict_proba(comment2_vect)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickling trained RandomForest models for all categories. \n",
    "\n",
    "#### We choose Random Forest instead of LinearSVC although the latter performs well, as RDF has predict_proba function and LinearSVC does not. We need to output a probability score for each comment, remember?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have to pickle not only the TF-IDF vectorizer object, but also the RDF model trained on the related vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def pickle_model(df, label):\n",
    "    \n",
    "    X = df.comment_text\n",
    "    y = df[label]\n",
    "\n",
    "    # Initiate a Tfidf vectorizer\n",
    "    tfv = TfidfVectorizer(ngram_range=(1,1), stop_words='english')\n",
    "    \n",
    "    # Convert the X data into a document term matrix dataframe\n",
    "    X_vect = tfv.fit_transform(X)  \n",
    "    \n",
    "    # saves the column labels (ie. the vocabulary)\n",
    "    # wb means Writing to the file in Binary mode, written in byte objects\n",
    "    with open(r\"{}.pkl\".format(label + '_vect'), \"wb\") as f:   \n",
    "        pickle.dump(tfv, f)   \n",
    "        \n",
    "    randomforest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    randomforest.fit(X_vect, y)\n",
    "\n",
    "    # Create a new pickle file based on random forest\n",
    "    with open(r\"{}.pkl\".format(label + '_model'), \"wb\") as f:  \n",
    "        pickle.dump(randomforest, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Pickle files\n",
    "\n",
    "datalist = [data_tox_done, data_sev_done, data_obs_done, data_ins_done, data_thr_done, data_ide_done]\n",
    "label = ['toxic', 'severe_toxic', 'obscene', 'insult', 'threat', 'identity_hate']\n",
    "\n",
    "for i,j in zip(datalist,label):\n",
    "    pickle_model(i, j)\n",
    "    \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9b2d5b3201222ec7ff21f91cd09ff578d7030c5c74759a0a8142994ecd9263e6"
  },
  "kernelspec": {
   "display_name": "'toxic'",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
